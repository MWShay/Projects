{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "T75jcje3NE10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu6-aKXVD43p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "404d8cba-765a-4672-df01-4f8d9172e093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "Device found : []\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "import json\n",
        "import h5py\n",
        "import imageio\n",
        "from IPython.display import Image\n",
        "import os\n",
        "\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "#from tifffile import imsave\n",
        "import h5py\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(f'Device found : {physical_device}')"
      ],
      "metadata": {
        "id": "BYQi_rcQzpUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "l4BpJOyCu-zh",
        "outputId": "02d26860-1deb-49b8-988a-a9c0f646997c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "HeadMRI_DIR = \"D:/DataSet/HeadSeg/Raw_HeadSet/MRI/\"\n",
        "Headlabel_DIR = \"D:/DataSet/HeadSeg/Raw_HeadSet/new_Label/\"\n",
        "AUGMENT_DIR = \"D:/DataSet/HeadSeg/Raw_HeadSet/Augmentation/\"\n",
        "\n",
        "TRAIN_DATA_DIR = \"/Volumes/MX500_1TB/DataSet/3DBrainTissueSegmentation/train/\"\n",
        "VALID_DATA_DIR = \"/Volumes/MX500_1TB/DataSet/3DBrainTissueSegmentation/valid/\"\n",
        "TEST_DATA_DIR = \"/Volumes/MX500_1TB/DataSet/3DBrainTissueSegmentation/test/\""
      ],
      "metadata": {
        "id": "x7umjbG5zwpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MRI_list = os.listdir(HeadMRI_DIR)\n",
        "Label_list = os.listdir(Headlabel_DIR)\n",
        "print(MRI_list)\n",
        "print(Label_list)"
      ],
      "metadata": {
        "id": "C4HCY9u7z_qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Equally Sliced Subvolume, Size(80,80,64)"
      ],
      "metadata": {
        "id": "5pliQife0W2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_x = 240\n",
        "orig_y = 240\n",
        "orig_z = 192\n",
        "step_x = 40\n",
        "step_y = 40\n",
        "step_z = 32\n",
        "output_x = 80\n",
        "output_y = 80\n",
        "output_z = 64\n",
        "num_classes = 6\n",
        "\n",
        "for i in range(0, len(MRI_list)):\n",
        "    filename = MRI_list[i]\n",
        "    img=np.array(nib.load(HeadMRI_DIR + MRI_list[i]).get_fdata())\n",
        "    msk=np.array(nib.load(Headlabel_DIR + Label_list[i]).get_fdata())\n",
        "    img_scale=scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
        "\n",
        "    #Reshape to 240x240x192\n",
        "    image = img_scale[0:240, 5:245, :]\n",
        "    label = msk[0:240, 5:245, :]\n",
        "\n",
        "        # Initialize features and labels with `None`\n",
        "    X = None\n",
        "    Y = None\n",
        "\n",
        "    for x in range(0, orig_x-output_x+1, step_x):\n",
        "        for y in range(0, orig_y-output_y+1, step_y):\n",
        "            for z in range(0, orig_z-output_z+1, step_z):\n",
        "\n",
        "                start_x = x\n",
        "                start_y = y\n",
        "                start_z = z\n",
        "\n",
        "                Y = label[start_x: start_x + output_x,start_y: start_y + output_y,start_z: start_z + output_z]\n",
        "\n",
        "                Y = keras.utils.to_categorical(Y, num_classes=num_classes)\n",
        "\n",
        "                X = np.copy(image[start_x: start_x + output_x,start_y: start_y + output_y,start_z: start_z + output_z])\n",
        "\n",
        "                    # change dimension of X\n",
        "                    # from (x_dim, y_dim, z_dim, num_channels)\n",
        "                    # to (num_channels, x_dim, y_dim, z_dim)\n",
        "                X = np.expand_dims(X, axis=0)\n",
        "\n",
        "                    # change dimension of y\n",
        "                    # from (x_dim, y_dim, z_dim, num_classes)\n",
        "                    # to (num_classes, x_dim, y_dim, z_dim)\n",
        "                Y = np.moveaxis(Y, 3, 0)\n",
        "\n",
        "                    #excludes the background class\n",
        "                Y = Y[1:, :, :, :]\n",
        "\n",
        "                file=filename \\\n",
        "                +\"_x_\"+str(start_x) \\\n",
        "                +\"_y_\"+str(start_y) \\\n",
        "                +\"_z_\"+str(start_z) \\\n",
        "                +\"_\"\n",
        "\n",
        "                import h5py\n",
        "\n",
        "                destination = os.path.join(\"D:/DataSet/HeadSeg/Raw_HeadSet\", f\"{file}.h5\")\n",
        "\n",
        "                if os.path.exists(destination):\n",
        "                    # File already exists, do something\n",
        "                    pass\n",
        "\n",
        "                else:\n",
        "                    os.makedirs(os.path.dirname(destination), exist_ok= True)\n",
        "                    with h5py.File(destination, \"w\") as f1:\n",
        "                        dset1 = f1.create_dataset(\"x\", (1, output_x, output_y, output_z), dtype='float64', data=X)\n",
        "                        dset2 = f1.create_dataset(\"y\", (5, output_x, output_y, output_z), dtype='uint8', data=Y)\n",
        "                        f1.close()\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "m25WYDJ80Kwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Random Subvolumn, Size(80,80,64) Must contain CSF, GM, WM"
      ],
      "metadata": {
        "id": "VIN_aF9G0RJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_sub_volume(image, label, filename = None,\n",
        "                   orig_x = 240, orig_y = 240, orig_z = 192,\n",
        "                   output_x = 80, output_y = 80, output_z = 64,\n",
        "                   num_classes = 6, max_tries = 100, brain_ratio = 0.8):\n",
        "\n",
        "    # Initialize features and labels with `None`\n",
        "    X = None\n",
        "    Y = None\n",
        "\n",
        "    tries = 0\n",
        "\n",
        "    while tries < max_tries:\n",
        "        # randomly sample sub-volume by sampling the corner voxel\n",
        "        # hint: make sure to leave enough room for the output dimensions!\n",
        "        start_x = np.random.randint(0, orig_x - output_x+1)\n",
        "        start_y = np.random.randint(0, orig_y - output_y+1)\n",
        "        start_z = np.random.randint(0, orig_z - output_z+1)\n",
        "\n",
        "        # extract relevant area of label\n",
        "        Y = label[start_x: start_x + output_x,\n",
        "                  start_y: start_y + output_y,\n",
        "                  start_z: start_z + output_z]\n",
        "\n",
        "        #print(Y.max())\n",
        "\n",
        "        #check the ratio of volume contains brain volume a.k.a CSF, GM, WM\n",
        "        brain = np.count_nonzero(np.array(Y) >= 3) / (80 * 80 * 64)\n",
        "        #print(\"brain is \" + f\"{brain}\")\n",
        "        # increment tries counter\n",
        "        tries += 1\n",
        "\n",
        "        # if background ratio is below the desired threshold,\n",
        "        # use that sub-volume.\n",
        "        # otherwise continue the loop and try another random sub-volume\n",
        "        if brain >= brain_ratio:\n",
        "\n",
        "            # One-hot encode the categories.\n",
        "            # This adds a 4th dimension, 'num_classes'\n",
        "            # (output_x, output_y, output_z, num_classes)\n",
        "            Y = keras.utils.to_categorical(Y, num_classes=num_classes)\n",
        "\n",
        "            # make copy of the sub-volume\n",
        "            X = np.copy(image[start_x: start_x + output_x,\n",
        "                              start_y: start_y + output_y,\n",
        "                              start_z: start_z + output_z])\n",
        "\n",
        "            # change dimension of X\n",
        "            # from (x_dim, y_dim, z_dim, num_channels)\n",
        "            # to (num_channels, x_dim, y_dim, z_dim)\n",
        "            #X = np.moveaxis(X, 3, 0)\n",
        "            X = np.expand_dims(X, axis=0)\n",
        "\n",
        "            # change dimension of y\n",
        "            # from (x_dim, y_dim, z_dim, num_classes)\n",
        "            # to (num_classes, x_dim, y_dim, z_dim)\n",
        "            Y = np.moveaxis(Y, 3, 0)\n",
        "\n",
        "            # take a subset of y that excludes the background class\n",
        "            # in the 'num_classes' dimension\n",
        "            Y = Y[1:, :, :, :]\n",
        "\n",
        "            file=filename \\\n",
        "                +\"_random\" \\\n",
        "                +\"_x_\"+str(start_x) \\\n",
        "                +\"_y_\"+str(start_y) \\\n",
        "                +\"_z_\"+str(start_z)\n",
        "\n",
        "            print(file + \" brain ratio: \" + str(brain))\n",
        "            print(f\"Image value maximum: {X.max()}, Image shape: {X.shape}\")\n",
        "            print(f\"Label value maximum: {Y.max()}, label shape: {X.shape}\")\n",
        "\n",
        "\n",
        "            return X, Y, file\n",
        "\n",
        "    # if we've tried max_tries number of samples\n",
        "    # Give up in order to avoid looping forever.\n",
        "    print(f\"Tried {tries} times to find a sub-volume. Giving up...\")"
      ],
      "metadata": {
        "id": "_GImOTnn0PsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_x = 80\n",
        "output_y = 80\n",
        "output_z = 64\n",
        "\n",
        "for i in range(0, len(MRI_list)):\n",
        "    file_name = MRI_list[i]\n",
        "    img=np.array(nib.load(HeadMRI_DIR + MRI_list[i]).get_fdata())\n",
        "    msk=np.array(nib.load(Headlabel_DIR + Label_list[i]).get_fdata())\n",
        "    img_scale=scaler.fit_transform(img.reshape(-1, img.shape[-1])).reshape(img.shape)\n",
        "\n",
        "    #Reshape to 240x240x192\n",
        "    #img = img_scale[0:240, 5:245, :]\n",
        "    #msk = msk[0:240, 5:245, :]\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    while count < 25:\n",
        "\n",
        "        X, Y , file = get_random_sub_volume(image = img_scale, label = msk, filename=file_name, brain_ratio=0.8)\n",
        "\n",
        "\n",
        "        destination = os.path.join(\"D:/DataSet/HeadSeg/Raw_HeadSet\", f\"{file}.h5\")\n",
        "\n",
        "        if os.path.exists(destination):\n",
        "            # File already exists, do something\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            os.makedirs(os.path.dirname(destination), exist_ok= True)\n",
        "            with h5py.File(destination, \"w\") as f1:\n",
        "                dset1 = f1.create_dataset(\"x\", (1, output_x, output_y, output_z), dtype='float64', data=X)\n",
        "                dset2 = f1.create_dataset(\"y\", (5, output_x, output_y, output_z), dtype='uint8', data=Y)\n",
        "                f1.close()\n",
        "\n",
        "        count += 1"
      ],
      "metadata": {
        "id": "zyP6xGAe0jEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sub_volume(image, label, filename,\n",
        "                   orig_x = 256, orig_y = 256, orig_z = 192,\n",
        "                   output_x = 160, output_y = 160, output_z = 16,\n",
        "                   num_classes = 6, max_tries = 1000,\n",
        "                   background_threshold=0.95):\n",
        "    \"\"\"\n",
        "    Extract random sub-volume from original images.\n",
        "\n",
        "    Args:\n",
        "        image (np.array): original image,\n",
        "            of shape (orig_x, orig_y, orig_z, num_channels)\n",
        "        label (np.array): original label.\n",
        "            labels coded using discrete values rather than\n",
        "            a separate dimension,\n",
        "            so this is of shape (orig_x, orig_y, orig_z)\n",
        "        orig_x (int): x_dim of input image\n",
        "        orig_y (int): y_dim of input image\n",
        "        orig_z (int): z_dim of input image\n",
        "        output_x (int): desired x_dim of output\n",
        "        output_y (int): desired y_dim of output\n",
        "        output_z (int): desired z_dim of output\n",
        "        num_classes (int): number of class labels\n",
        "        max_tries (int): maximum trials to do when sampling\n",
        "        background_threshold (float): limit on the fraction\n",
        "            of the sample which can be the background\n",
        "\n",
        "    returns:\n",
        "        X (np.array): sample of original image of dimension\n",
        "            (num_channels, output_x, output_y, output_z)\n",
        "        y (np.array): labels which correspond to X, of dimension\n",
        "            (num_classes, output_x, output_y, output_z)\n",
        "    \"\"\"\n",
        "    # Initialize features and labels with `None`\n",
        "    X = None\n",
        "    y = None\n",
        "\n",
        "    tries = 0\n",
        "    while tries < max_tries:\n",
        "        # randomly sample sub-volume by sampling the corner voxel\n",
        "        # hint: make sure to leave enough room for the output dimensions!\n",
        "        start_x = np.random.randint(0, orig_x - output_x+1)\n",
        "        start_y = np.random.randint(0, orig_y - output_y+1)\n",
        "        start_z = np.random.randint(0, orig_z - output_z+1)\n",
        "\n",
        "        # extract relevant area of label\n",
        "        y = label[start_x: start_x + output_x,\n",
        "                  start_y: start_y + output_y,\n",
        "                  start_z: start_z + output_z]\n",
        "\n",
        "        # One-hot encode the categories.\n",
        "        # This adds a 4th dimension, 'num_classes'\n",
        "        # (output_x, output_y, output_z, num_classes)\n",
        "        y = keras.utils.to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "        # compute the background ratio\n",
        "        background_ratio = np.sum(y[:, :, :, 0])/(output_x * output_y * output_z)\n",
        "\n",
        "        # increment tries counter\n",
        "        tries += 1\n",
        "\n",
        "        # if background ratio is below the desired threshold,\n",
        "        # use that sub-volume.\n",
        "        # otherwise continue the loop and try another random sub-volume\n",
        "        if background_ratio < background_threshold:\n",
        "\n",
        "            # make copy of the sub-volume\n",
        "            X = np.copy(image[start_x: start_x + output_x,\n",
        "                              start_y: start_y + output_y,\n",
        "                              start_z: start_z + output_z])\n",
        "\n",
        "            # change dimension of X\n",
        "            # from (x_dim, y_dim, z_dim, num_channels)\n",
        "            # to (num_channels, x_dim, y_dim, z_dim)\n",
        "            #X = np.moveaxis(X, 3, 0)\n",
        "            X = np.expand_dims(X, axis=0)\n",
        "\n",
        "            # change dimension of y\n",
        "            # from (x_dim, y_dim, z_dim, num_classes)\n",
        "            # to (num_classes, x_dim, y_dim, z_dim)\n",
        "            y = np.moveaxis(y, 3, 0)\n",
        "\n",
        "            # take a subset of y that excludes the background class\n",
        "            # in the 'num_classes' dimension\n",
        "            y = y[1:, :, :, :]\n",
        "\n",
        "            file=filename \\\n",
        "                +\"x\"+str(start_x) \\\n",
        "                +\"y\"+str(start_y) \\\n",
        "                +\"z\"+str(start_z)\n",
        "\n",
        "            return X, y, file\n",
        "\n",
        "    # if we've tried max_tries number of samples\n",
        "    # Give up in order to avoid looping forever.\n",
        "    print(f\"Tried {tries} times to find a sub-volume. Giving up...\")\n",
        "\n",
        "\n",
        "# set home directory and data directory\n",
        "#HOME_DIR = \"./BraTS-Data/\"\n",
        "HOME_DIR = \"/Users/mwshay/Library/CloudStorage/OneDrive-UNSW/ThesisProject/jupyternotebook/Headset_Data/\"\n",
        "MRI_DIR = HOME_DIR + \"MRI/\"\n",
        "LABEL_DIR = HOME_DIR + \"Label/\"\n",
        "\n",
        "def load_case(image_nifty_file, label_nifty_file):\n",
        "    # load the image and label file, get the image content and return a numpy array for each\n",
        "    image = np.array(nib.load(image_nifty_file).get_fdata())\n",
        "    label = np.array(nib.load(label_nifty_file).get_fdata())\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def standardize(image):\n",
        "    \"\"\"\n",
        "    Standardize mean and standard deviation\n",
        "        of each channel and z_dimension.\n",
        "\n",
        "    Args:\n",
        "        image (np.array): input image,\n",
        "            shape (num_channels, dim_x, dim_y, dim_z)\n",
        "\n",
        "    Returns:\n",
        "        standardized_image (np.array): standardized version of input image\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "    # initialize to array of zeros, with same shape as the image\n",
        "    standardized_image = np.zeros(image.shape)\n",
        "    # iterate over channels\n",
        "    for c in range(image.shape[0]):\n",
        "        # iterate over the `z` dimension\n",
        "        for z in range(image.shape[3]):\n",
        "            # get a slice of the image\n",
        "            # at channel c and z-th dimension `z`\n",
        "            image_slice = image[c,:,:,z]\n",
        "\n",
        "            # subtract the mean from image_slice\n",
        "            centered = image_slice - np.mean(image_slice)\n",
        "\n",
        "            # divide by the standard deviation (only if it is different from zero)\n",
        "            if np.std(centered) != 0:\n",
        "                centered_scaled = centered / np.std(centered)\n",
        "\n",
        "                # update  the slice of standardized image\n",
        "                # with the scaled centered and scaled image\n",
        "            standardized_image[c, :, :, z] = centered_scaled\n",
        "\n",
        "    return standardized_image"
      ],
      "metadata": {
        "id": "UxCBordSu_kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "HOME_DIR = \"D:/DataSet/HeadSeg/\"\n",
        "Train_DIR = HOME_DIR + \"train/\"\n",
        "Valid_DIR = HOME_DIR + \"valid/\"\n",
        "Test_DIR = HOME_DIR + \"test/\"\n",
        "\n",
        "def get_file_names(path):\n",
        "    files = []\n",
        "    for filename in os.listdir(path):\n",
        "        if os.path.isfile(os.path.join(path, filename)):\n",
        "            files.append(filename)\n",
        "    return files\n",
        "\n",
        "train_files = get_file_names(Train_DIR)\n",
        "valid_files = get_file_names(Valid_DIR)\n",
        "config = {\"train\": train_files,\"valid\":valid_files}\n",
        "print(config)\n",
        "\n",
        "SAVE_DIR = HOME_DIR\n",
        "os.chdir(SAVE_DIR)\n",
        "\n",
        "with open('config.json', 'w') as f:\n",
        "    json.dump(config, f)\n",
        "    f.close()\n",
        "\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "w7XDOu_gzKPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}